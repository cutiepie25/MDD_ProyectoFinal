# -*- coding: utf-8 -*-
"""Entrenamiento_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KtU4W0jeiClmXFMJeqrbF5GiJCSi542B
"""

#Zona de importe primeras librerías
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

# Cargamos los datos
# Cargar el archivo CSV
data = pd.read_csv('data_prepared.csv')
pd.set_option('display.float_format', '{:.2f}'.format)

data.head()

#División 70-30 para validación cruzada
X = data.drop("Pay Amt", axis=1)
Y = data['Pay Amt']

# División estratégica: 70% para entrenamiento (con CV) y 30% para prueba final
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.3, random_state=42
)
Y_train.plot(kind='box')

"""### ***Validación Cruzada***"""

# #Validación Cruzada
# from sklearn.model_selection import cross_validate

# #Dataframe para comparar los modelos
# comparacion_CV=pd.DataFrame()

# #Medidas de evaluación
# scoring=('neg_mean_absolute_error', 'neg_mean_squared_error','neg_root_mean_squared_error', 'neg_mean_absolute_percentage_error')

# #Muestreo lineal
# cv=10

# Configuración de K-Fold con 10 particiones
kfold = KFold(n_splits=10, shuffle=True, random_state=42)

# DataFrame para almacenar resultados de validación cruzada
resultados_cv = pd.DataFrame()

# DataFrame para almacenar resultados en conjunto de prueba
medidas_prueba = pd.DataFrame(index=['mse', 'rmse', 'mae', 'mape', 'r2', 'max_error'])

# #Método de ML a usar en la validación cruzada
# from sklearn import tree
# modelTree = tree.DecisionTreeRegressor(criterion='squared_error', min_samples_leaf=20, max_depth=10)


# scores = cross_validate(modelTree, X, Y, cv=cv, scoring=scoring, return_train_score=True, return_estimator=False)
# scores=pd.DataFrame(scores) #Se almacenan los resultados en un dataframe
# scores

# # Promedios para verificar overfitting comparando medidas en train y test
# scores.mean()

# #Cuidado overfitting, más de 5 puntos de diferencia

from sklearn.tree import DecisionTreeRegressor

model_dt = DecisionTreeRegressor(
    criterion='squared_error',
    max_depth=10,
    min_samples_leaf=20,
    min_samples_split=50,
    random_state=42
)

# Validación cruzada
scores_cv = cross_val_score(
    model_dt, X_train, Y_train,
    cv=kfold,
    scoring='neg_mean_squared_error'
)
resultados_cv['Arbol_DT'] = -scores_cv

# Entrenamiento con el conjunto completo de entrenamiento
model_dt.fit(X_train, Y_train)

# Evaluación en conjunto de prueba
Y_pred = model_dt.predict(X_test)

mse = metrics.mean_squared_error(Y_test, Y_pred)
rmse = np.sqrt(mse)
mae = metrics.mean_absolute_error(Y_test, Y_pred)
mape = metrics.mean_absolute_percentage_error(Y_test, Y_pred)
r2 = metrics.r2_score(Y_test, Y_pred)
max_error = metrics.max_error(Y_test, Y_pred)

medidas_prueba['Arbol_DT'] = [mse, rmse, mae, mape, r2, max_error]
medidas_prueba

from sklearn.tree import plot_tree
plt.figure(figsize=(30,30))
plot_tree(model_dt, feature_names=X_train.columns.values,  rounded=True, filled=True)
plt.show()

from sklearn.neighbors import KNeighborsRegressor

model_knn = KNeighborsRegressor(
    n_neighbors=5,
    metric='euclidean',
    weights='distance'
)

# Validación cruzada con datos escalados
scores_cv = cross_val_score(
    model_knn, X_train, Y_train,
    cv=kfold,
    scoring='neg_mean_squared_error'
)
resultados_cv['KNN'] = -scores_cv

# Entrenamiento
model_knn.fit(X_train, Y_train)

# Evaluación en conjunto de prueba
Y_pred = model_knn.predict(X_test)

mse = metrics.mean_squared_error(Y_test, Y_pred)
rmse = np.sqrt(mse)
mae = metrics.mean_absolute_error(Y_test, Y_pred)
mape = metrics.mean_absolute_percentage_error(Y_test, Y_pred)
r2 = metrics.r2_score(Y_test, Y_pred)
max_error = metrics.max_error(Y_test, Y_pred)

medidas_prueba['KNN'] = [mse, rmse, mae, mape, r2, max_error]
medidas_prueba

from sklearn.neural_network import MLPRegressor

model_nn = MLPRegressor(
    hidden_layer_sizes=(100, 50),
    activation='relu',
    solver='adam',
    learning_rate='adaptive',
    learning_rate_init=0.001,
    max_iter=500,
    random_state=42,
    verbose=False,
    early_stopping=True
)

# Validación cruzada con datos escalados
scores_cv = cross_val_score(
    model_nn, X_train, Y_train,
    cv=kfold,
    scoring='neg_mean_squared_error'
)
resultados_cv['Red_Neuronal'] = -scores_cv

# Entrenamiento
model_nn.fit(X_train, Y_train)

# Evaluación en conjunto de prueba
Y_pred = model_nn.predict(X_test)

mse = metrics.mean_squared_error(Y_test, Y_pred)
rmse = np.sqrt(mse)
mae = metrics.mean_absolute_error(Y_test, Y_pred)
mape = metrics.mean_absolute_percentage_error(Y_test, Y_pred)
r2 = metrics.r2_score(Y_test, Y_pred)
max_error = metrics.max_error(Y_test, Y_pred)

medidas_prueba['Red_Neuronal'] = [mse, rmse, mae, mape, r2, max_error]
medidas_prueba

from sklearn.svm import SVR

model_svm = SVR(
    kernel='rbf',
    C=1.0,
    epsilon=0.1,
    gamma='scale'
)

# Validación cruzada con datos escalados
scores_cv = cross_val_score(
    model_svm, X_train, Y_train,
    cv=kfold,
    scoring='neg_mean_squared_error'
)
resultados_cv['SVM'] = -scores_cv

# Entrenamiento
model_svm.fit(X_train, Y_train)

# Evaluación en conjunto de prueba
Y_pred = model_svm.predict(X_test)

mse = metrics.mean_squared_error(Y_test, Y_pred)
rmse = np.sqrt(mse)
mae = metrics.mean_absolute_error(Y_test, Y_pred)
mape = metrics.mean_absolute_percentage_error(Y_test, Y_pred)
r2 = metrics.r2_score(Y_test, Y_pred)
max_error = metrics.max_error(Y_test, Y_pred)

medidas_prueba['SVM'] = [mse, rmse, mae, mape, r2, max_error]
medidas_prueba